{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a85aba0",
   "metadata": {},
   "source": [
    "# **Workflow for Sentinel-1 TOPS Coregistrastion Stack**\n",
    "\n",
    "> This workflow is based on the Sentinel-1 TOPS Coregistrastion Processing Stack in ISCE2, which is designed for Dolphin processing.\n",
    " \n",
    "**Minimum Requirements:**\n",
    "- [install ISCE2](https://github.com/InSARProcessing/isce2/blob/develop/isce%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0.md)\n",
    "- [Sentinel-1 TOPS data](https://search.asf.alaska.edu/#/)\n",
    "- Local SRTM GL1 DEM tiles -> You don't have to unzip them.\n",
    "\n",
    "**Steps:** </br>\n",
    "- Download TOPS precise orbit files </br>\n",
    "- Download auxliary calibration files </br>\n",
    "- Prepare SRTM GL1 mosaic DEM </br>\n",
    "- Stack TOPS data </br>\n",
    "- Generate configs </br>\n",
    "- Run ISCE2 stack </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2f7a3",
   "metadata": {},
   "source": [
    "üõ†Ô∏èBefore the stack starts, we need to configure the environment variables and the paths for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fdc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# 1. Input ISCE repository location\n",
    "isce_install = \"/home/wang_xuanxuan/tools/isce2\"\n",
    "\n",
    "os.environ[\"PATH\"] = os.path.join(isce_install, \"install\", \"bin\") + \":\" + os.environ[\"PATH\"]\n",
    "# Check if the PYTHONPATH exists, if not, initialize it as an empty string\n",
    "pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(isce_install, \"install\", \"packages\") + \":\" + pythonpath\n",
    "\n",
    "# 2. Input workspace and meta data location\n",
    "WORKSPACE = \"/mnt/e/InSAR/Stuttgart/ISCE\"          # The workspace prepared for ISCE2 processing\n",
    "SLC_LOCATION = \"/mnt/e/InSAR/Stuttgart/RAW\"         # The location of Sentinel-1 TOPS data\n",
    "LOCAL_DEM_DIR = \"/mnt/e/DEM/SRTMGL1/\"               # The location which you stored all the local SRTM GL1 DEM tiles\n",
    "\n",
    "# Then automatically set up the orbit and auxliary calibration files directory\n",
    "ORBIT_DIR = os.path.join(WORKSPACE, \"ORB\")\n",
    "AUX_DIR = os.path.join(WORKSPACE, \"AUX\")\n",
    "os.makedirs(WORKSPACE, exist_ok=True)\n",
    "os.makedirs(ORBIT_DIR, exist_ok=True)\n",
    "os.makedirs(AUX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ee6be",
   "metadata": {},
   "source": [
    "üëÄ This is a double-check step to confirm the SLC files for processing, provided by the variable `SLC_LOCATION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16de654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 SAFE files\n"
     ]
    }
   ],
   "source": [
    "# Locate all SAFE files in the directory\n",
    "safe_files = glob.glob(os.path.join(SLC_LOCATION, 'S1*_IW_SLC*'))\n",
    "if not safe_files:\n",
    "    print(f\"No SAFE files found in directory: {SLC_LOCATION}\")\n",
    "\n",
    "print(f\"Found {len(safe_files)} SAFE files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb5851",
   "metadata": {},
   "source": [
    "Now the setting is done. Let's start the workflow ! ‚ò∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a4aa2",
   "metadata": {},
   "source": [
    "<b> 1. Download precise orbit files </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def _get_orbit_list() -> list[str]:\n",
    "    \"\"\"\n",
    "    ‰ªé ASFÔºàAlaska Satellite FacilityÔºâÊúçÂä°Âô®Ëé∑Âèñ Sentinel-1 Âç´ÊòüËΩ®ÈÅìÊñá‰ª∂ÂàóË°®„ÄÇ\n",
    "    \n",
    "    ËØ•ÂáΩÊï∞‰ªé ASF ÁöÑËΩ®ÈÅìÊñá‰ª∂Â≠òÂÇ®Â∫ìËé∑ÂèñÊâÄÊúâÂèØÁî®ÁöÑÁ≤æÂØÜËΩ®ÈÅìÊñá‰ª∂Ôºà.EOF Ê†ºÂºèÔºâÂàóË°®„ÄÇ\n",
    "    Ëøô‰∫õËΩ®ÈÅìÊñá‰ª∂ÂåÖÂê´Âç´ÊòüÁöÑÁ≤æÁ°Æ‰ΩçÁΩÆ‰ø°ÊÅØÔºåÁî®‰∫é InSAR Êï∞ÊçÆÂ§ÑÁêÜ‰∏≠ÁöÑÂá†‰ΩïÊ†°Ê≠£ÂíåÈÖçÂáÜ„ÄÇ\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: ËΩ®ÈÅìÊñá‰ª∂ÂêçÂàóË°®Ôºà.EOF Ê†ºÂºèÔºâ„ÄÇÂ¶ÇÊûúËé∑ÂèñÂ§±Ë¥•ÔºåËøîÂõûÁ©∫ÂàóË°®„ÄÇ\n",
    "    \"\"\"\n",
    "    url_root = \"https://s1qc.asf.alaska.edu/aux_poeorb\"\n",
    "\n",
    "    try:\n",
    "        session = _create_session()\n",
    "        response = session.get(url_root, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # print(response.text)\n",
    "        # Extract all .EOF files from the HTML\n",
    "        orbit_files = re.findall(r'href=\"([^\"]*\\.EOF)\"', response.text)\n",
    "        print(f\"Found {len(orbit_files)} orbit files.\")\n",
    "        return orbit_files\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get orbit list: {e}\")\n",
    "        return []\n",
    "\n",
    "def _create_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    ÂàõÂª∫‰∏Ä‰∏™Â∏¶ÊúâÈáçËØïÁ≠ñÁï•ÁöÑ requests ‰ºöËØù„ÄÇ\n",
    "    \n",
    "    ÈÖçÁΩÆ‰∫ÜËá™Âä®ÈáçËØïÊú∫Âà∂ÔºåÁî®‰∫éÂ§ÑÁêÜÁΩëÁªúËØ∑Ê±Ç‰∏≠ÁöÑÂ∏∏ËßÅÈîôËØØÔºàÂ¶ÇÊúçÂä°Âô®ÈîôËØØ„ÄÅË∂ÖÊó∂Á≠âÔºâÔºå\n",
    "    ÊèêÈ´ò‰∏ãËΩΩËΩ®ÈÅìÊñá‰ª∂ÁöÑÂèØÈù†ÊÄß„ÄÇ\n",
    "    \n",
    "    Returns:\n",
    "        requests.Session: ÈÖçÁΩÆÂ•ΩÈáçËØïÁ≠ñÁï•ÁöÑ‰ºöËØùÂØπË±°„ÄÇ\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Setup retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    return session\n",
    "\n",
    "# Download Precise Orbit\n",
    "from datetime import datetime, timedelta\n",
    "orbit_list = _get_orbit_list()\n",
    "if not orbit_list:\n",
    "    print(\"Failed to get orbit file list from ASF server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11792e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_orbit_file_asf(orbit_file: str, output_path: os.PathLike) -> bool:\n",
    "    url_root = \"https://s1qc.asf.alaska.edu/aux_poeorb\"\n",
    "    file_url = f\"{url_root}/{orbit_file}\"\n",
    "\n",
    "    try:\n",
    "        session = _create_session()\n",
    "        response = session.get(file_url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Get file size\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded_size = 0\n",
    "\n",
    "        with open(output_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    downloaded_size += len(chunk)\n",
    "\n",
    "                    # Show progress\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded_size / total_size) * 100\n",
    "                        print(f\"\\r  Progress: {progress:.1f}%\", end='', flush=True)\n",
    "\n",
    "        print()  # New line after progress\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  Download failed: {e}\")\n",
    "        # Clean up partially downloaded file\n",
    "        if output_path.exists():\n",
    "            output_path.unlink()\n",
    "        return False\n",
    "\n",
    "print(f\"Start downloading orbit files from ASF...\")\n",
    "# For each SAFE file, download the corresponding orbit file\n",
    "for safe_file in safe_files:\n",
    "    print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "    print(f\"Processing: {os.path.basename(safe_file)}\")\n",
    "    \n",
    "    # Extract mission and timestamp from SAFE \n",
    "    basename = os.path.basename(safe_file)\n",
    "\n",
    "    # Remove .SAFE or .zip extension\n",
    "    if basename.endswith('.SAFE'):\n",
    "        basename = basename[:-5]\n",
    "    elif basename.endswith('.zip'):\n",
    "        basename = basename[:-4]\n",
    "\n",
    "    # Split by underscores\n",
    "    parts = basename.split('_')\n",
    "\n",
    "    if len(parts) < 6:\n",
    "        print(f\"  Invalid filename format: {basename}\")\n",
    "        continue\n",
    "\n",
    "    # Mission identifier (S1A or S1B)\n",
    "    mission = parts[0]\n",
    "\n",
    "    # Timestamps are typically at positions 5 and 6\n",
    "    # Format: YYYYMMDDTHHMMSS\n",
    "    start_time_str = parts[5]\n",
    "    stop_time_str = parts[6]\n",
    "\n",
    "    # Parse timestamps\n",
    "    try:\n",
    "        start_time = datetime.strptime(start_time_str, '%Y%m%dT%H%M%S')\n",
    "        stop_time = datetime.strptime(stop_time_str, '%Y%m%dT%H%M%S')\n",
    "    except ValueError:\n",
    "        print(f\"  Could not parse timestamps from: {basename}\")\n",
    "        continue\n",
    "\n",
    "    if not mission or not start_time:\n",
    "        print(\"  Could not parse SAFE filename, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Calculate date range for orbit search (one day before and after)\n",
    "    date1 = (start_time - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    date2 = (start_time + timedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "    # Find matching orbit file\n",
    "    mission_short = mission[-1]\n",
    "    # Search for matching orbit file\n",
    "    for orbit_file in orbit_list:\n",
    "    # Check if file matches mission and contains both dates\n",
    "        if (f\"S1{mission_short}\" in orbit_file and\n",
    "            date1 in orbit_file and date2 in orbit_file):\n",
    "            # Check if orbit file already exists in orbit directory\n",
    "            orbit_path = os.path.join(ORBIT_DIR, orbit_file)\n",
    "            if os.path.exists(orbit_path):\n",
    "                print(f\"  Orbit file already exists: {orbit_file}\")\n",
    "                continue\n",
    "            # Download orbit file\n",
    "            success = _download_orbit_file_asf(orbit_file, orbit_path)\n",
    "\n",
    "            if success:\n",
    "                print(f\"  Successfully downloaded: {orbit_file}\")\n",
    "            else:\n",
    "                print(f\"  Failed to download orbit file for: {os.path.basename(safe_file)}\")\n",
    "    \n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished downloading orbit files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff347626",
   "metadata": {},
   "source": [
    "<b> 2. Download AUX files </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4310ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download aux\n",
    "import subprocess\n",
    "print(f\"Start downloading auxiliary files from SAR-MPC...\")\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "for slc_file in safe_files:\n",
    "    filename = os.path.basename(slc_file)\n",
    "    wget_cmd = ['wget', 'https://sar-mpc.eu/download/ca97845e-1314-4817-91d8-f39afbeff74d/',\n",
    "                '-O', filename]\n",
    "    aux_file = os.path.join(AUX_DIR, filename)\n",
    "    if os.path.exists(aux_file):\n",
    "        print(f\"  Auxiliary file already exists: {aux_file}\")\n",
    "        print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "        continue\n",
    "    subprocess.run(wget_cmd, cwd=AUX_DIR, check=True)\n",
    "    print(f\"Downloaded auxiliary file for {filename} to {AUX_DIR}. \\n\")\n",
    "    print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "\n",
    "print(f\"Finished downloading auxiliary files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56856372",
   "metadata": {},
   "source": [
    "<b> 3. Download DEM </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing DEM ...\n",
      "* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\n",
      "\n",
      "Executing: dem.py -a stitch -b 47 51 7 12 -r -s 1 -c -l -d /mnt/e/DEM/SRTMGL1/\n",
      "Using default ISCE Path: /home/wang_xuanxuan/tools/isce2/install/packages/isce\n",
      "Could not create a stitched DEM. Some tiles are missing\n",
      "N50E007.SRTMGL1.hgt.zip = failed\n",
      "N50E008.SRTMGL1.hgt.zip = failed\n",
      "N50E009.SRTMGL1.hgt.zip = failed\n",
      "N50E010.SRTMGL1.hgt.zip = failed\n",
      "N50E011.SRTMGL1.hgt.zip = failed\n",
      "N49E007.SRTMGL1.hgt.zip = failed\n",
      "N49E008.SRTMGL1.hgt.zip = failed\n",
      "N49E009.SRTMGL1.hgt.zip = failed\n",
      "N49E010.SRTMGL1.hgt.zip = failed\n",
      "N49E011.SRTMGL1.hgt.zip = failed\n",
      "N48E007.SRTMGL1.hgt.zip = failed\n",
      "N48E008.SRTMGL1.hgt.zip = failed\n",
      "N48E009.SRTMGL1.hgt.zip = failed\n",
      "N48E010.SRTMGL1.hgt.zip = failed\n",
      "N48E011.SRTMGL1.hgt.zip = failed\n",
      "N47E007.SRTMGL1.hgt.zip = failed\n",
      "N47E008.SRTMGL1.hgt.zip = failed\n",
      "N47E009.SRTMGL1.hgt.zip = failed\n",
      "N47E010.SRTMGL1.hgt.zip = failed\n",
      "N47E011.SRTMGL1.hgt.zip = failed\n",
      "* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\n",
      "\n",
      "Finished downloading and mosaicing DEM - /mnt/e/DEM/SRTMGL1/demLat_N47_N51_Lon_E007_E012.dem.wgs84\n"
     ]
    }
   ],
   "source": [
    "# Download DEM\n",
    "import zipfile\n",
    "# First, customize a function to detect the lat/lon region of the SLCs\n",
    "lon_list = []\n",
    "lat_list = []\n",
    "polygons = []\n",
    "print(f\"Start preparing DEM ...\")\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "for safe_file in safe_files:\n",
    "    if safe_file.endswith(\".SAFE\"):\n",
    "        try:\n",
    "            # For unzipped SAFE files, find the KML file in the preview directory\n",
    "            kml_file = glob.glob(os.path.join(safe_file, \"preview\", f\"map-overlay.kml\"))\n",
    "            if kml_file:\n",
    "                with open(kml_file[0], 'r', encoding='utf-8') as kml:  # Fixed variable name and encoding\n",
    "                    content = kml.read()\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error extracting KML from SAFE file {safe_file}: {e}\")\n",
    "        \n",
    "    elif safe_file.endswith(\".zip\"):\n",
    "        # For zipped SAFE files, directly read the KML file from inside the ZIP\n",
    "        try:\n",
    "            with zipfile.ZipFile(safe_file, 'r') as zip_ref:\n",
    "                # Search for the map-overlay.kml file in the preview directory\n",
    "                kml_entries = [entry for entry in zip_ref.namelist() if \"preview/map-overlay.kml\" in entry]\n",
    "                if kml_entries:\n",
    "                    # Read the KML content directly from the ZIP file\n",
    "                    with zip_ref.open(kml_entries[0]) as kml_file:\n",
    "                        content = kml_file.read().decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error processing ZIP file {safe_file}: {e}\")\n",
    "\n",
    "    coord_pattern = r'<coordinates[^>]*>(.*?)</coordinates>'\n",
    "    coord_matches = re.findall(coord_pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    for coord_block in coord_matches:\n",
    "        points = []\n",
    "        coord_text = coord_block.strip()\n",
    "        coord_text = re.sub(r'\\s+', ' ', coord_text)\n",
    "        coord_triplets = coord_text.split()\n",
    "\n",
    "        for coord_str in coord_triplets:\n",
    "            if ',' in coord_str:\n",
    "                parts = coord_str.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    try:\n",
    "                        lon = float(parts[0])\n",
    "                        lat = float(parts[1])\n",
    "                        if -180 <= lon <= 180 and -90 <= lat <= 90:\n",
    "                            points.append((lon, lat))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "    if points:\n",
    "        # Remove duplicate closing point if exists\n",
    "        if len(points) > 1 and points[0] == points[-1]:\n",
    "            points = points[:-1]\n",
    "        polygons.append(points)\n",
    "    \n",
    "if polygons and len(polygons) > 0:\n",
    "    lons = [pt[0] for pt in polygons[0]]\n",
    "    lats = [pt[1] for pt in polygons[0]]\n",
    "    lon_list.extend(lon for lon in lons)\n",
    "    lat_list.extend(lat for lat in lats)\n",
    "\n",
    "# Find the common lat/lon region of the SLCs\n",
    "DEM_REGION = (int(min(lat_list)), int(max(lat_list)) + 1, int(min(lon_list)), int(max(lon_list)) + 1)\n",
    "\n",
    "os.chdir(LOCAL_DEM_DIR)\n",
    "dem_cmd = f\"dem.py -a stitch -b {DEM_REGION[0]} {DEM_REGION[1]} {DEM_REGION[2]} {DEM_REGION[3]} -r -s 1 -c -k -l -d {LOCAL_DEM_DIR}\"\n",
    "print(f\"Executing: {dem_cmd}\")\n",
    "! {dem_cmd}\n",
    "dem_output = os.path.join(LOCAL_DEM_DIR, f\"demLat_N{DEM_REGION[0]:02d}_N{DEM_REGION[1]:02d}_Lon_E{DEM_REGION[2]:03d}_E{DEM_REGION[3]:03d}.dem.wgs84\")\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished downloading and mosaicing DEM - {dem_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d07a1f",
   "metadata": {},
   "source": [
    "<b> 4. Generate configs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b855085",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(isce_install, \"src\", \"isce2\", \"contrib\", \"stack\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(isce_install, \"src\", \"isce2\", \"contrib\", \"stack\", \"topsStack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(WORKSPACE, \"run_files\")):\n",
    "    for file in os.listdir(os.path.join(WORKSPACE, \"run_files\")):\n",
    "        os.remove(os.path.join(WORKSPACE, \"run_files\", file))\n",
    "    os.rmdir(os.path.join(WORKSPACE, \"run_files\"))\n",
    "# Run stackSentinel.py\n",
    "!stackSentinel.py -s {SLC_LOCATION} -o {ORBIT_DIR} -a {AUX_DIR} -d {dem_output} -w {WORKSPACE} -c 3 -r 10 -z 2 -m 20250109 -C NESD -W slc -useGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f8060",
   "metadata": {},
   "source": [
    "<b> 5. Run steps (all in one) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(isce_install, \"src\", \"isce2\", \"contrib\", \"stack\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(isce_install, \"src\", \"isce2\", \"contrib\", \"stack\", \"topsStack\")\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "run_files_dir = os.path.join(WORKSPACE, \"run_files\")\n",
    "for file in os.listdir(run_files_dir):\n",
    "    cmd = f\"bash ./run_files/{file} 2>&1 | tee -a ./run_all.log\"\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c240486",
   "metadata": {},
   "source": [
    "<b> 6. (Alternative) Run steps (one by one) </b></br>\n",
    "This is the alternative way to run the steps one by one, in order to understand the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_01_unpack_topo_reference 2>&1 | tee ./run_01_unpack_topo_reference.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished unpacking topo reference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_02_unpack_secondary_slc 2>&1 | tee ./run_02_unpack_secondary_slc.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished unpacking secondary SLCs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_03_average_baseline 2>&1 | tee ./run_03_average_baseline.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished averaging baseline.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e814f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_04_extract_burst_overlaps 2>&1 | tee ./run_04_extract_burst_overlaps.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished extracting burst overlaps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fe78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_05_overlap_geo2rdr 2>&1 | tee ./run_05_overlap_geo2rdr.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished geo2rdr for overlap bursts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804df999",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_06_overlap_resample 2>&1 | tee ./run_06_overlap_resample.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished resampling overlap bursts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f21653",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_07_pairs_misreg 2>&1 | tee ./run_07_pairs_misreg.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished misregistration for pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c571ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_08_timeseries_misreg 2>&1 | tee ./run_08_timeseries_misreg.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished misregistration for timeseries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_09_fullBurst_geo2rdr 2>&1 | tee ./run_09_fullBurst_geo2rdr.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished geo2rdr for full bursts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_10_fullBurst_resample 2>&1 | tee ./run_10_fullBurst_resample.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished resampling full bursts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66490bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_11_extract_stack_valid_region 2>&1 | tee ./run_11_extract_stack_valid_region.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished extracting stack valid region.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dc528",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_12_merge_reference_secondary_slc 2>&1 | tee ./run_12_merge_reference_secondary_slc.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished merging reference and secondary SLC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!bash ./run_files/run_13_grid_baseline 2>&1 | tee ./run_13_grid_baseline.log\n",
    "print(\"* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */\\n\")\n",
    "print(f\"Finished gridding baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d892a2",
   "metadata": {},
   "source": [
    "That's all for the SLC Coregistration process in ISCE2. Now you can check the results in the `WORKSPACE` directory. Enjoy it ÔºöÔºâÔºâÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db037",
   "metadata": {},
   "source": [
    "Now, for **Dolphin** use, an automatic process is designed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "# In case, jax-cuda is not configured right for the Dolphin use, \n",
    "!dolphin config \\\n",
    "    --cslc merged/SLC/*/*.slc.full \\\n",
    "    --worker-settings.no-gpu-enabled \\\n",
    "    --worker-settings.threads-per-worker 4 \\\n",
    "    --unwrap-options.unwrap-method ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKSPACE)\n",
    "!dolphin run dolphin_config.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isce2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
