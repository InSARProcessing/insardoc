{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41687e20",
   "metadata": {},
   "source": [
    "Source:https://github.com/ASFHyP3/hyp3-docs.git\n",
    "\n",
    "---\n",
    "# **Hyp3 InSAR preprocessing + Mintpy general workflow**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f59e30",
   "metadata": {},
   "source": [
    "### **1. Project setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "# Set parameters\n",
    "project_name = 'Ganzi'\n",
    "# work_dir = Path.cwd() / project_name\n",
    "work_dir = Path(\"/mnt/e/InSAR/Sentinel\")\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir = work_dir / 'data'\n",
    "\n",
    "stack_start = parse_date('2024-09-28 00:00:00Z')\n",
    "stack_end = parse_date('2024-12-31 00:00:00Z')\n",
    "max_temporal_baseline = 40  # days\n",
    "\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0d0ed",
   "metadata": {},
   "source": [
    "### **2. Add ASF search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13015da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def _extract_kml_polygons(file_path: str):\n",
    "    \"\"\"Extract polygon coordinates from KML file\"\"\"\n",
    "    polygons = []\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.kml'):\n",
    "            with open(file_path) as kml:\n",
    "                content = kml.read()\n",
    "                if isinstance(content, bytes):\n",
    "                    content = content.decode('utf-8', errors='ignore')\n",
    "                coord_pattern = r'<coordinates[^>]*>(.*?)</coordinates>'\n",
    "                coord_matches = re.findall(coord_pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "                for coord_block in coord_matches:\n",
    "                    points = []\n",
    "                    coord_text = coord_block.strip()\n",
    "                    coord_text = re.sub(r'\\s+', ' ', coord_text)\n",
    "                    coord_triplets = coord_text.split()\n",
    "\n",
    "                    for coord_str in coord_triplets:\n",
    "                        if ',' in coord_str:\n",
    "                            parts = coord_str.split(',')\n",
    "                            if len(parts) >= 2:\n",
    "                                try:\n",
    "                                    lon = float(parts[0])\n",
    "                                    lat = float(parts[1])\n",
    "                                    if -180 <= lon <= 180 and -90 <= lat <= 90:\n",
    "                                        points.append((lon, lat))\n",
    "                                except ValueError:\n",
    "                                    pass\n",
    "                    if points:\n",
    "                        # Remove duplicate closing point if exists\n",
    "                        if len(points) > 1 and points[0] == points[-1]:\n",
    "                            points = points[:-1]\n",
    "                        polygons.append(points)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting KML: {e}\")\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Select AOI with ASF search\n",
    "polygons = _extract_kml_polygons(f'/mnt/e/InSAR/Ganzi/crop.kml')\n",
    "aoi_wkt = Polygon(polygons[0]).wkt\n",
    "\n",
    "search_results = asf.geo_search(\n",
    "    platform=asf.PLATFORM.SENTINEL1,\n",
    "    # intersectsWith='POINT(-117.55 35.77)',\n",
    "    intersectsWith=aoi_wkt,\n",
    "    start='2024-09-01',\n",
    "    end='2024-12-31',\n",
    "    processingLevel=asf.PRODUCT_TYPE.SLC,\n",
    "    beamMode=asf.BEAMMODE.IW\n",
    ")\n",
    "# Baseline search\n",
    "baseline_results = asf.baseline_search.stack_from_product(search_results[-1])\n",
    "\n",
    "columns = list(baseline_results[0].properties.keys()) + ['geometry', ]\n",
    "data = [list(scene.properties.values()) + [scene.geometry, ] for scene in baseline_results]\n",
    "\n",
    "stack = pd.DataFrame(data, columns=columns)\n",
    "stack['startTime'] = stack.startTime.apply(parse_date)\n",
    "\n",
    "stack = stack.loc[(stack_start <= stack.startTime) & (stack.startTime <= stack_end)]\n",
    "\n",
    "# Set up pairs\n",
    "sbas_pairs = set()\n",
    "\n",
    "for reference, rt in stack.loc[::-1, ['sceneName', 'temporalBaseline']].itertuples(index=False):\n",
    "    secondaries = stack.loc[\n",
    "        (stack.sceneName != reference)\n",
    "        & (stack.temporalBaseline - rt <= max_temporal_baseline)\n",
    "        & (stack.temporalBaseline - rt > 0)\n",
    "    ]\n",
    "    for secondary in secondaries.sceneName:\n",
    "        sbas_pairs.add((reference, secondary))\n",
    "\n",
    "print(f\"Find {len(sbas_pairs)} SBAS pairs:\\n\")\n",
    "for i in sbas_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82318e9",
   "metadata": {},
   "source": [
    "### **3. Task running on Hyp3 server**\n",
    "---\n",
    "Notice: Before starting connection, set up `~/.netrc` as follows,\n",
    "```bash\n",
    "machine urs.earthdata.nasa.gov login [your_user_name] password [your_password]\n",
    "```\n",
    "Then run the following command to set up the permission of the file\n",
    "```bash\n",
    "chmod 600 ~/.netrc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c66f71",
   "metadata": {},
   "source": [
    "3.1 Connect to Hyp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b72e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyp3_sdk import HyP3, Batch, util\n",
    "hyp3 = HyP3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b75eb3",
   "metadata": {},
   "source": [
    "3.2 Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = Batch()\n",
    "for reference, secondary in sbas_pairs:\n",
    "    jobs += hyp3.submit_insar_job(reference, secondary, name=project_name,\n",
    "                                  include_dem=True, include_look_vectors=True)\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0243e",
   "metadata": {},
   "source": [
    "3.3 Watch the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.watch(jobs, timeout=8*3600, interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.find_jobs(name=project_name)\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a39d6",
   "metadata": {},
   "source": [
    "3.4 Download the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_products = jobs.download_files(data_dir)\n",
    "insar_products = [util.extract_zipped_product(ii) for ii in insar_products]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81095ed",
   "metadata": {},
   "source": [
    "### **4. Prepare for Mintpy time-series analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a11c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "    \n",
    "    Arg:\n",
    "        file_list: a list of GeoTIFF files\n",
    "    \n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in file_list]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data_dir.glob('*/*_dem.tif')\n",
    "\n",
    "overlap = get_common_overlap(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    files_for_mintpy = ['_water_mask.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_hyp3_products_to_common_overlap(data_dir, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mintpy_config = work_dir / 'mintpy_config.txt'\n",
    "mintpy_config.write_text(\n",
    "f\"\"\"# vim: set filetype=cfg:\n",
    "##------------------------  MintPy ----------------------------------##\n",
    "########## 1. load_data\n",
    "mintpy.load.processor        = hyp3\n",
    "##---------interferogram datasets\n",
    "mintpy.load.unwFile          = {data_dir}/*/*_unw_phase_clipped.tif\n",
    "mintpy.load.corFile          = {data_dir}/*/*_corr_clipped.tif\n",
    "##---------geometry datasets:\n",
    "mintpy.load.demFile          = {data_dir}/*/*_dem_clipped.tif\n",
    "mintpy.load.incAngleFile     = {data_dir}/*/*_lv_theta_clipped.tif\n",
    "mintpy.load.azAngleFile      = {data_dir}/*/*_lv_phi_clipped.tif\n",
    "mintpy.load.waterMaskFile    = {data_dir}/*/*_water_mask_clipped.tif\n",
    "\n",
    "########## 2. modify_network\n",
    "mintpy.network.coherenceBased       = yes       #[yes / no], auto for yes, exclude interferograms with coherence < minCoherence\n",
    "mintpy.network.keepMinSpanTree      = yes\n",
    "mintpy.network.minCoherence    = 0.4  #[0.0-1.0], auto for 0.7\n",
    "\n",
    "########## 5. invert_network\n",
    "mintpy.networkInversion.minTempCoh  = 0.5 #[0.0-1.0], auto for 0.7, min temporal coherence for mask\n",
    "\n",
    "########## 8. correct_troposphere (optional but recommended)\n",
    "mintpy.troposphericDelay.method = height_correlation  #[pyaps / height_correlation / gacos / no], auto for pyaps\n",
    "\n",
    "########## 9. deramp (optional)\n",
    "mintpy.deramp          = linear  #[no / linear / quadratic], auto for no - no ramp will be removed\n",
    "\n",
    "########## 11.2 reference_date\n",
    "## Reference all time-series to one date in time\n",
    "## reference: Yunjun et al. (2019, section 4.9)\n",
    "## no     - do not change the default reference date (1st date)\n",
    "#mintpy.reference.date =  no #[reference_date.txt / 20090214 / no], auto for reference_date.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d52ad",
   "metadata": {},
   "source": [
    "### **5. Run Mintpy time-series analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966121c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py --dir {work_dir} {mintpy_config}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyp3-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
